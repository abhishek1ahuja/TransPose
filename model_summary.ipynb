{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4974cc8a-cab0-4d1b-8a16-bd347f45a847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/abhishek/data/college/thesis/thesis_with_dr_duc/conda_envs/dlpruning5/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tptools import _init_paths\n",
    "import argparse\n",
    "from config import cfg\n",
    "from config import update_config\n",
    "from utils.utils import create_logger\n",
    "import pprint\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import models\n",
    "import dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from utils.utils import get_model_summary\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b748361-07f2-452c-aaaf-067889440712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(args___):\n",
    "    parser = argparse.ArgumentParser(description='Train keypoints network')\n",
    "    parser.add_argument('--cfg',\n",
    "                        help='experiment configure file name',\n",
    "                        required=True,\n",
    "                        type=str)\n",
    "\n",
    "\n",
    "    parser.add_argument('opts',\n",
    "                        help=\"Modify config options using the command-line\",\n",
    "                        default=None,\n",
    "                        nargs=argparse.REMAINDER)\n",
    "\n",
    "    parser.add_argument('--modelDir',\n",
    "                    help='model directory',\n",
    "                    type=str,\n",
    "                    default='')\n",
    "\n",
    "    parser.add_argument('--logDir',\n",
    "                    help='log directory',\n",
    "                type=str,\n",
    "                default='')\n",
    "    parser.add_argument('--dataDir',\n",
    "                        help='data directory',\n",
    "                        type=str,\n",
    "                        default='')\n",
    "    \n",
    "    args = parser.parse_args(args___)\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "355c4bd2-009f-4455-92ed-9c786752c7a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Namespace(cfg='experiments/coco/transpose_r/exp2/exp2_step1_prune.yaml', opts=['TEST.USE_GT_BBOX', 'True'], modelDir='', logDir='', dataDir='')\n",
      "AUTO_RESUME: True\n",
      "CUDNN:\n",
      "  BENCHMARK: True\n",
      "  DETERMINISTIC: False\n",
      "  ENABLED: True\n",
      "DATASET:\n",
      "  COLOR_RGB: True\n",
      "  DATASET: coco\n",
      "  DATA_FORMAT: jpg\n",
      "  FLIP: True\n",
      "  HYBRID_JOINTS_TYPE: \n",
      "  NUM_JOINTS_HALF_BODY: 8\n",
      "  PROB_HALF_BODY: 0.3\n",
      "  ROOT: data/coco/\n",
      "  ROT_FACTOR: 45\n",
      "  SCALE_FACTOR: 0.35\n",
      "  SELECT_DATA: False\n",
      "  TEST_SET: val2017\n",
      "  TRAIN_SET: train2017\n",
      "DATA_DIR: \n",
      "DEBUG:\n",
      "  DEBUG: False\n",
      "  SAVE_BATCH_IMAGES_GT: True\n",
      "  SAVE_BATCH_IMAGES_PRED: True\n",
      "  SAVE_HEATMAPS_GT: True\n",
      "  SAVE_HEATMAPS_PRED: True\n",
      "GPUS: (0,)\n",
      "LOG_DIR: log\n",
      "LOSS:\n",
      "  TOPK: 8\n",
      "  USE_DIFFERENT_JOINTS_WEIGHT: False\n",
      "  USE_OHKM: False\n",
      "  USE_TARGET_WEIGHT: True\n",
      "MODEL:\n",
      "  ATTENTION_ACTIVATION: relu\n",
      "  BOTTLENECK_NUM: 0\n",
      "  DIM_FEEDFORWARD: 1024\n",
      "  DIM_MODEL: 256\n",
      "  ENCODER_LAYERS: 4\n",
      "  EXTRA:\n",
      "    DECONV_WITH_BIAS: False\n",
      "    FINAL_CONV_KERNEL: 1\n",
      "    NUM_DECONV_FILTERS: [256]\n",
      "    NUM_DECONV_KERNELS: [4]\n",
      "    NUM_DECONV_LAYERS: 1\n",
      "    NUM_LAYERS: 50\n",
      "  HEATMAP_SIZE: [48, 64]\n",
      "  IMAGE_SIZE: [192, 256]\n",
      "  INIT_WEIGHTS: True\n",
      "  INTERMEDIATE_SUP: False\n",
      "  NAME: transpose_r_chsel\n",
      "  NUM_JOINTS: 17\n",
      "  NW_CFG: None\n",
      "  N_HEAD: 8\n",
      "  PE_ONLY_AT_BEGIN: False\n",
      "  POS_EMBEDDING: sine\n",
      "  PRETRAINED: models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      "  PRETRAINED_CHSEL: \n",
      "  SIGMA: 2\n",
      "  TAG_PER_JOINT: True\n",
      "  TARGET_TYPE: gaussian\n",
      "OUTPUT_DIR: output\n",
      "PIN_MEMORY: True\n",
      "PRINT_FREQ: 100\n",
      "RANK: 0\n",
      "TEST:\n",
      "  BATCH_SIZE_PER_GPU: 8\n",
      "  BBOX_THRE: 1.0\n",
      "  BLUR_KERNEL: 11\n",
      "  COCO_BBOX_FILE: data/coco/person_detection_results/COCO_val2017_detections_AP_H_56_person.json\n",
      "  FLIP_TEST: True\n",
      "  IMAGE_THRE: 0.0\n",
      "  IN_VIS_THRE: 0.2\n",
      "  MODEL_FILE: models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      "  NMS_THRE: 1.0\n",
      "  OKS_THRE: 0.9\n",
      "  POST_PROCESS: True\n",
      "  SHIFT_HEATMAP: True\n",
      "  SOFT_NMS: False\n",
      "  USE_GT_BBOX: True\n",
      "TRAIN:\n",
      "  BATCH_SIZE_PER_GPU: 8\n",
      "  BEGIN_EPOCH: 0\n",
      "  CHECKPOINT: \n",
      "  END_EPOCH: 230\n",
      "  GAMMA1: 0.99\n",
      "  GAMMA2: 0.0\n",
      "  LR: 0.0001\n",
      "  LR_END: 1e-05\n",
      "  LR_FACTOR: 0.25\n",
      "  LR_STEP: [100, 150, 200, 220]\n",
      "  MOMENTUM: 0.9\n",
      "  NESTEROV: False\n",
      "  OPTIMIZER: adam\n",
      "  RESUME: False\n",
      "  SHUFFLE: True\n",
      "  WD: 0.0001\n",
      "WORKERS: 1\n",
      "==> Add Sine PositionEmbedding~\n",
      "=> init final conv weights from normal distribution\n",
      "=> init .weight as normal(0, 0.001)\n",
      "=> init .bias as 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> creating output/coco/transpose_r_chsel/exp2_step1_prune\n",
      "=> creating log/coco/transpose_r_chsel/exp2_step1_prune_2024-06-26-17-45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=> loading pretrained model models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: pos_embedding is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: conv1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: bn1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: bn1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: bn1.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: bn1.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: bn1.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.conv1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.bn1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.bn1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.bn1.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.bn1.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.bn1.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.conv2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.bn2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.bn2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.bn2.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.bn2.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.bn2.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.conv3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.bn3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.bn3.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.bn3.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.bn3.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.bn3.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.downsample.0.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.downsample.1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.downsample.1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.downsample.1.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.downsample.1.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.downsample.1.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.1.conv1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.1.bn1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.1.bn1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.1.bn1.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.1.bn1.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.1.bn1.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.1.conv2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.1.bn2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.1.bn2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.1.bn2.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.1.bn2.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.1.bn2.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.1.conv3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.1.bn3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.1.bn3.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.1.bn3.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.1.bn3.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.1.bn3.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.2.conv1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.2.bn1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.2.bn1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.2.bn1.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.2.bn1.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.2.bn1.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.2.conv2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.2.bn2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.2.bn2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.2.bn2.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.2.bn2.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.2.bn2.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.2.conv3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.2.bn3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.2.bn3.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.2.bn3.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.2.bn3.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.2.bn3.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.conv1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.bn1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.bn1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.bn1.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.bn1.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.bn1.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.conv2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.bn2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.bn2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.bn2.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.bn2.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.bn2.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.conv3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.bn3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.bn3.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.bn3.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.bn3.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.bn3.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.downsample.0.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.downsample.1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.downsample.1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.downsample.1.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.downsample.1.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.downsample.1.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.1.conv1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.1.bn1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.1.bn1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.1.bn1.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.1.bn1.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.1.bn1.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.1.conv2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.1.bn2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.1.bn2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.1.bn2.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.1.bn2.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.1.bn2.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.1.conv3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.1.bn3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.1.bn3.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.1.bn3.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.1.bn3.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.1.bn3.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.2.conv1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.2.bn1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.2.bn1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.2.bn1.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.2.bn1.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.2.bn1.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.2.conv2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.2.bn2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.2.bn2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.2.bn2.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.2.bn2.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.2.bn2.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.2.conv3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.2.bn3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.2.bn3.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.2.bn3.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.2.bn3.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.2.bn3.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.3.conv1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.3.bn1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.3.bn1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.3.bn1.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.3.bn1.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.3.bn1.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.3.conv2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.3.bn2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.3.bn2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.3.bn2.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.3.bn2.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.3.bn2.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.3.conv3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.3.bn3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.3.bn3.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.3.bn3.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.3.bn3.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.3.bn3.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: reduce.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.0.self_attn.in_proj_weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.0.self_attn.in_proj_bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.0.self_attn.out_proj.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.0.self_attn.out_proj.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.0.linear1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.0.linear1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.0.linear2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.0.linear2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.0.norm1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.0.norm1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.0.norm2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.0.norm2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.1.self_attn.in_proj_weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.1.self_attn.in_proj_bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.1.self_attn.out_proj.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.1.self_attn.out_proj.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.1.linear1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.1.linear1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.1.linear2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.1.linear2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.1.norm1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.1.norm1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.1.norm2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.1.norm2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.2.self_attn.in_proj_weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.2.self_attn.in_proj_bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.2.self_attn.out_proj.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.2.self_attn.out_proj.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.2.linear1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.2.linear1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.2.linear2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.2.linear2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.2.norm1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.2.norm1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.2.norm2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.2.norm2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.3.self_attn.in_proj_weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.3.self_attn.in_proj_bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.3.self_attn.out_proj.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.3.self_attn.out_proj.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.3.linear1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.3.linear1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.3.linear2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.3.linear2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.3.norm1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.3.norm1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.3.norm2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.3.norm2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: deconv_layers.0.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: deconv_layers.1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: deconv_layers.1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: deconv_layers.1.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: deconv_layers.1.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: deconv_layers.1.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: final_layer.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: final_layer.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n"
     ]
    }
   ],
   "source": [
    "config_file_path = 'experiments/coco/transpose_r/exp2/exp2_step1_prune.yaml'\n",
    "args = parse_args(['--cfg', config_file_path, 'TEST.USE_GT_BBOX', 'True'])\n",
    "update_config(cfg, args)\n",
    "\n",
    "logger, final_output_dir, tb_log_dir = create_logger(cfg, args.cfg, 'valid')\n",
    "\n",
    "logger.info(pprint.pformat(args))\n",
    "logger.info(cfg)\n",
    "\n",
    "cudnn.benchmark = cfg.CUDNN.BENCHMARK\n",
    "torch.backends.cudnn.deterministic = cfg.CUDNN.DETERMINISTIC\n",
    "torch.backends.cudnn.enabled = cfg.CUDNN.ENABLED\n",
    "\n",
    "\n",
    "model = eval('models.'+cfg.MODEL.NAME+'.get_pose_net')(\n",
    "    cfg, is_train=False\n",
    ")\n",
    "model.init_weights(cfg.TEST.MODEL_FILE)\n",
    "model = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fc87d46-8555-45fe-b7c8-dc5fa9e07d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.TEST.MODEL_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c281adf-aa80-4867-a4f1-ae6860af1ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 64, 128, 96]           9,408\n",
      "       BatchNorm2d-2          [-1, 64, 128, 96]             128\n",
      "              ReLU-3          [-1, 64, 128, 96]               0\n",
      "         MaxPool2d-4           [-1, 64, 64, 48]               0\n",
      "            Conv2d-5           [-1, 64, 64, 48]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 64, 48]             128\n",
      "  ChannelSelection-7           [-1, 64, 64, 48]               0\n",
      "              ReLU-8           [-1, 64, 64, 48]               0\n",
      "            Conv2d-9           [-1, 64, 64, 48]          36,864\n",
      "      BatchNorm2d-10           [-1, 64, 64, 48]             128\n",
      "             ReLU-11           [-1, 64, 64, 48]               0\n",
      "           Conv2d-12          [-1, 256, 64, 48]          16,384\n",
      "      BatchNorm2d-13          [-1, 256, 64, 48]             512\n",
      "           Conv2d-14          [-1, 256, 64, 48]          16,384\n",
      "      BatchNorm2d-15          [-1, 256, 64, 48]             512\n",
      "             ReLU-16          [-1, 256, 64, 48]               0\n",
      "       Bottleneck-17          [-1, 256, 64, 48]               0\n",
      "           Conv2d-18           [-1, 64, 64, 48]          16,384\n",
      "      BatchNorm2d-19           [-1, 64, 64, 48]             128\n",
      " ChannelSelection-20           [-1, 64, 64, 48]               0\n",
      "             ReLU-21           [-1, 64, 64, 48]               0\n",
      "           Conv2d-22           [-1, 64, 64, 48]          36,864\n",
      "      BatchNorm2d-23           [-1, 64, 64, 48]             128\n",
      "             ReLU-24           [-1, 64, 64, 48]               0\n",
      "           Conv2d-25          [-1, 256, 64, 48]          16,384\n",
      "      BatchNorm2d-26          [-1, 256, 64, 48]             512\n",
      "             ReLU-27          [-1, 256, 64, 48]               0\n",
      "       Bottleneck-28          [-1, 256, 64, 48]               0\n",
      "           Conv2d-29           [-1, 64, 64, 48]          16,384\n",
      "      BatchNorm2d-30           [-1, 64, 64, 48]             128\n",
      " ChannelSelection-31           [-1, 64, 64, 48]               0\n",
      "             ReLU-32           [-1, 64, 64, 48]               0\n",
      "           Conv2d-33           [-1, 64, 64, 48]          36,864\n",
      "      BatchNorm2d-34           [-1, 64, 64, 48]             128\n",
      "             ReLU-35           [-1, 64, 64, 48]               0\n",
      "           Conv2d-36          [-1, 256, 64, 48]          16,384\n",
      "      BatchNorm2d-37          [-1, 256, 64, 48]             512\n",
      "             ReLU-38          [-1, 256, 64, 48]               0\n",
      "       Bottleneck-39          [-1, 256, 64, 48]               0\n",
      "           Conv2d-40          [-1, 128, 64, 48]          32,768\n",
      "      BatchNorm2d-41          [-1, 128, 64, 48]             256\n",
      " ChannelSelection-42          [-1, 128, 64, 48]               0\n",
      "             ReLU-43          [-1, 128, 64, 48]               0\n",
      "           Conv2d-44          [-1, 128, 32, 24]         147,456\n",
      "      BatchNorm2d-45          [-1, 128, 32, 24]             256\n",
      "             ReLU-46          [-1, 128, 32, 24]               0\n",
      "           Conv2d-47          [-1, 512, 32, 24]          65,536\n",
      "      BatchNorm2d-48          [-1, 512, 32, 24]           1,024\n",
      "           Conv2d-49          [-1, 512, 32, 24]         131,072\n",
      "      BatchNorm2d-50          [-1, 512, 32, 24]           1,024\n",
      "             ReLU-51          [-1, 512, 32, 24]               0\n",
      "       Bottleneck-52          [-1, 512, 32, 24]               0\n",
      "           Conv2d-53          [-1, 128, 32, 24]          65,536\n",
      "      BatchNorm2d-54          [-1, 128, 32, 24]             256\n",
      " ChannelSelection-55          [-1, 128, 32, 24]               0\n",
      "             ReLU-56          [-1, 128, 32, 24]               0\n",
      "           Conv2d-57          [-1, 128, 32, 24]         147,456\n",
      "      BatchNorm2d-58          [-1, 128, 32, 24]             256\n",
      "             ReLU-59          [-1, 128, 32, 24]               0\n",
      "           Conv2d-60          [-1, 512, 32, 24]          65,536\n",
      "      BatchNorm2d-61          [-1, 512, 32, 24]           1,024\n",
      "             ReLU-62          [-1, 512, 32, 24]               0\n",
      "       Bottleneck-63          [-1, 512, 32, 24]               0\n",
      "           Conv2d-64          [-1, 128, 32, 24]          65,536\n",
      "      BatchNorm2d-65          [-1, 128, 32, 24]             256\n",
      " ChannelSelection-66          [-1, 128, 32, 24]               0\n",
      "             ReLU-67          [-1, 128, 32, 24]               0\n",
      "           Conv2d-68          [-1, 128, 32, 24]         147,456\n",
      "      BatchNorm2d-69          [-1, 128, 32, 24]             256\n",
      "             ReLU-70          [-1, 128, 32, 24]               0\n",
      "           Conv2d-71          [-1, 512, 32, 24]          65,536\n",
      "      BatchNorm2d-72          [-1, 512, 32, 24]           1,024\n",
      "             ReLU-73          [-1, 512, 32, 24]               0\n",
      "       Bottleneck-74          [-1, 512, 32, 24]               0\n",
      "           Conv2d-75          [-1, 128, 32, 24]          65,536\n",
      "      BatchNorm2d-76          [-1, 128, 32, 24]             256\n",
      " ChannelSelection-77          [-1, 128, 32, 24]               0\n",
      "             ReLU-78          [-1, 128, 32, 24]               0\n",
      "           Conv2d-79          [-1, 128, 32, 24]         147,456\n",
      "      BatchNorm2d-80          [-1, 128, 32, 24]             256\n",
      "             ReLU-81          [-1, 128, 32, 24]               0\n",
      "           Conv2d-82          [-1, 512, 32, 24]          65,536\n",
      "      BatchNorm2d-83          [-1, 512, 32, 24]           1,024\n",
      "             ReLU-84          [-1, 512, 32, 24]               0\n",
      "       Bottleneck-85          [-1, 512, 32, 24]               0\n",
      "           Conv2d-86          [-1, 256, 32, 24]         131,072\n",
      "MultiheadAttention-87  [[-1, 2, 256], [-1, 768, 768]]               0\n",
      "          Dropout-88               [-1, 2, 256]               0\n",
      "        LayerNorm-89               [-1, 2, 256]             512\n",
      "           Linear-90              [-1, 2, 1024]         263,168\n",
      "          Dropout-91              [-1, 2, 1024]               0\n",
      "           Linear-92               [-1, 2, 256]         262,400\n",
      "          Dropout-93               [-1, 2, 256]               0\n",
      "        LayerNorm-94               [-1, 2, 256]             512\n",
      "TransformerEncoderLayer-95               [-1, 2, 256]               0\n",
      "MultiheadAttention-96  [[-1, 2, 256], [-1, 768, 768]]               0\n",
      "          Dropout-97               [-1, 2, 256]               0\n",
      "        LayerNorm-98               [-1, 2, 256]             512\n",
      "           Linear-99              [-1, 2, 1024]         263,168\n",
      "         Dropout-100              [-1, 2, 1024]               0\n",
      "          Linear-101               [-1, 2, 256]         262,400\n",
      "         Dropout-102               [-1, 2, 256]               0\n",
      "       LayerNorm-103               [-1, 2, 256]             512\n",
      "TransformerEncoderLayer-104               [-1, 2, 256]               0\n",
      "MultiheadAttention-105  [[-1, 2, 256], [-1, 768, 768]]               0\n",
      "         Dropout-106               [-1, 2, 256]               0\n",
      "       LayerNorm-107               [-1, 2, 256]             512\n",
      "          Linear-108              [-1, 2, 1024]         263,168\n",
      "         Dropout-109              [-1, 2, 1024]               0\n",
      "          Linear-110               [-1, 2, 256]         262,400\n",
      "         Dropout-111               [-1, 2, 256]               0\n",
      "       LayerNorm-112               [-1, 2, 256]             512\n",
      "TransformerEncoderLayer-113               [-1, 2, 256]               0\n",
      "MultiheadAttention-114  [[-1, 2, 256], [-1, 768, 768]]               0\n",
      "         Dropout-115               [-1, 2, 256]               0\n",
      "       LayerNorm-116               [-1, 2, 256]             512\n",
      "          Linear-117              [-1, 2, 1024]         263,168\n",
      "         Dropout-118              [-1, 2, 1024]               0\n",
      "          Linear-119               [-1, 2, 256]         262,400\n",
      "         Dropout-120               [-1, 2, 256]               0\n",
      "       LayerNorm-121               [-1, 2, 256]             512\n",
      "TransformerEncoderLayer-122               [-1, 2, 256]               0\n",
      "TransformerEncoder-123               [-1, 2, 256]               0\n",
      " ConvTranspose2d-124          [-1, 256, 64, 48]       1,048,576\n",
      "     BatchNorm2d-125          [-1, 256, 64, 48]             512\n",
      "            ReLU-126          [-1, 256, 64, 48]               0\n",
      "          Conv2d-127           [-1, 17, 64, 48]           4,369\n",
      "================================================================\n",
      "Total params: 4,735,825\n",
      "Trainable params: 4,735,825\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.56\n",
      "Forward/backward pass size (MB): 8976.88\n",
      "Params size (MB): 18.07\n",
      "Estimated Total Size (MB): 8995.51\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (3, 256, 192))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117f99e2-da22-40aa-9e3e-c65c05f921fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
