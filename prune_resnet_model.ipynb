{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed0f4a96-3b09-4b8b-bca6-9576362ec908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import tools._init_paths\n",
    "from config import cfg\n",
    "from config import update_config\n",
    "from core.loss import JointsMSELoss\n",
    "from core.function import validate\n",
    "from utils.utils import create_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aec49e73-f5ab-4f13-b1f3-995fb11ac639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset\n",
    "import models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6f40f72-8314-4ed9-bb45-f217a0807184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9e79c4f-32e7-458d-904c-2cbe59ae73f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca6301b4-3e6a-4658-94b1-8843eadc34df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(args___):\n",
    "    parser = argparse.ArgumentParser(description='Train keypoints network')\n",
    "    # general\n",
    "    parser.add_argument('--cfg',\n",
    "                        help='experiment configure file name',\n",
    "                        required=True,\n",
    "                        type=str)\n",
    "\n",
    "    parser.add_argument('opts',\n",
    "                        help=\"Modify config options using the command-line\",\n",
    "                        default=None,\n",
    "                        nargs=argparse.REMAINDER)\n",
    "\n",
    "    parser.add_argument('--modelDir',\n",
    "                        help='model directory',\n",
    "                        type=str,\n",
    "                        default='')\n",
    "    parser.add_argument('--logDir',\n",
    "                        help='log directory',\n",
    "                        type=str,\n",
    "                        default='')\n",
    "    parser.add_argument('--dataDir',\n",
    "                        help='data directory',\n",
    "                        type=str,\n",
    "                        default='')\n",
    "    parser.add_argument('--prevModelDir',\n",
    "                        help='prev Model directory',\n",
    "                        type=str,\n",
    "                        default='')\n",
    "\n",
    "    args = parser.parse_args(args___)\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d13c660-f5b1-462e-b6bf-e82765b1b096",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = 'experiments/coco/transpose_r/TP_R_256x192_d256_h1024_enc4_mh8.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9033446a-f585-4733-8018-4a7ab71b51ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_file_path = 'experiments/coco/transpose_h/TP_H_w32_256x192_stage3_1_4_d64_h128_relu_enc4_mh1.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3edce75-80ec-45d9-b31d-d6f159577411",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args(['--cfg', config_file_path, 'TEST.USE_GT_BBOX', 'True'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bc8e5be-abb2-40a3-add0-637d16788e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_config(cfg, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fcd3587-9ebe-4662-91ae-c2fe515f0caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CfgNode({'OUTPUT_DIR': 'output', 'LOG_DIR': 'log', 'DATA_DIR': '', 'GPUS': (0,), 'WORKERS': 24, 'PRINT_FREQ': 100, 'AUTO_RESUME': True, 'PIN_MEMORY': True, 'RANK': 0, 'CUDNN': CfgNode({'BENCHMARK': True, 'DETERMINISTIC': False, 'ENABLED': True}), 'MODEL': CfgNode({'NAME': 'transpose_r', 'INIT_WEIGHTS': True, 'PRETRAINED': 'models/pytorch/imagenet/resnet50-19c8e357.pth', 'NUM_JOINTS': 17, 'TAG_PER_JOINT': True, 'TARGET_TYPE': 'gaussian', 'IMAGE_SIZE': [192, 256], 'HEATMAP_SIZE': [48, 64], 'SIGMA': 2, 'EXTRA': CfgNode({'FINAL_CONV_KERNEL': 1, 'DECONV_WITH_BIAS': False, 'NUM_DECONV_LAYERS': 1, 'NUM_DECONV_FILTERS': [256], 'NUM_DECONV_KERNELS': [4], 'NUM_LAYERS': 50}), 'BOTTLENECK_NUM': 0, 'DIM_MODEL': 256, 'DIM_FEEDFORWARD': 1024, 'ENCODER_LAYERS': 4, 'N_HEAD': 8, 'ATTENTION_ACTIVATION': 'relu', 'POS_EMBEDDING': 'sine', 'INTERMEDIATE_SUP': False, 'PE_ONLY_AT_BEGIN': False}), 'LOSS': CfgNode({'USE_OHKM': False, 'TOPK': 8, 'USE_TARGET_WEIGHT': True, 'USE_DIFFERENT_JOINTS_WEIGHT': False}), 'DATASET': CfgNode({'ROOT': 'data/coco/', 'DATASET': 'coco', 'TRAIN_SET': 'train2017', 'TEST_SET': 'val2017', 'DATA_FORMAT': 'jpg', 'HYBRID_JOINTS_TYPE': '', 'SELECT_DATA': False, 'FLIP': True, 'SCALE_FACTOR': 0.35, 'ROT_FACTOR': 45, 'PROB_HALF_BODY': 0.3, 'NUM_JOINTS_HALF_BODY': 8, 'COLOR_RGB': True}), 'TRAIN': CfgNode({'LR_FACTOR': 0.25, 'LR_STEP': [100, 150, 200, 220], 'LR': 0.0001, 'LR_END': 1e-05, 'OPTIMIZER': 'adam', 'MOMENTUM': 0.9, 'WD': 0.0001, 'NESTEROV': False, 'GAMMA1': 0.99, 'GAMMA2': 0.0, 'BEGIN_EPOCH': 0, 'END_EPOCH': 230, 'RESUME': False, 'CHECKPOINT': '', 'BATCH_SIZE_PER_GPU': 20, 'SHUFFLE': True}), 'TEST': CfgNode({'BLUR_KERNEL': 11, 'BATCH_SIZE_PER_GPU': 8, 'FLIP_TEST': True, 'POST_PROCESS': True, 'SHIFT_HEATMAP': True, 'USE_GT_BBOX': True, 'IMAGE_THRE': 0.0, 'NMS_THRE': 1.0, 'SOFT_NMS': False, 'OKS_THRE': 0.9, 'IN_VIS_THRE': 0.2, 'COCO_BBOX_FILE': 'data/coco/person_detection_results/COCO_val2017_detections_AP_H_56_person.json', 'BBOX_THRE': 1.0, 'MODEL_FILE': 'models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth'}), 'DEBUG': CfgNode({'DEBUG': False, 'SAVE_BATCH_IMAGES_GT': True, 'SAVE_BATCH_IMAGES_PRED': True, 'SAVE_HEATMAPS_GT': True, 'SAVE_HEATMAPS_PRED': True})})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a51846db-e07c-4a6d-b7a0-2f4daaddb02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Namespace(cfg='experiments/coco/transpose_r/TP_R_256x192_d256_h1024_enc4_mh8.yaml', opts=['TEST.USE_GT_BBOX', 'True'], modelDir='', logDir='', dataDir='', prevModelDir='')\n",
      "AUTO_RESUME: True\n",
      "CUDNN:\n",
      "  BENCHMARK: True\n",
      "  DETERMINISTIC: False\n",
      "  ENABLED: True\n",
      "DATASET:\n",
      "  COLOR_RGB: True\n",
      "  DATASET: coco\n",
      "  DATA_FORMAT: jpg\n",
      "  FLIP: True\n",
      "  HYBRID_JOINTS_TYPE: \n",
      "  NUM_JOINTS_HALF_BODY: 8\n",
      "  PROB_HALF_BODY: 0.3\n",
      "  ROOT: data/coco/\n",
      "  ROT_FACTOR: 45\n",
      "  SCALE_FACTOR: 0.35\n",
      "  SELECT_DATA: False\n",
      "  TEST_SET: val2017\n",
      "  TRAIN_SET: train2017\n",
      "DATA_DIR: \n",
      "DEBUG:\n",
      "  DEBUG: False\n",
      "  SAVE_BATCH_IMAGES_GT: True\n",
      "  SAVE_BATCH_IMAGES_PRED: True\n",
      "  SAVE_HEATMAPS_GT: True\n",
      "  SAVE_HEATMAPS_PRED: True\n",
      "GPUS: (0,)\n",
      "LOG_DIR: log\n",
      "LOSS:\n",
      "  TOPK: 8\n",
      "  USE_DIFFERENT_JOINTS_WEIGHT: False\n",
      "  USE_OHKM: False\n",
      "  USE_TARGET_WEIGHT: True\n",
      "MODEL:\n",
      "  ATTENTION_ACTIVATION: relu\n",
      "  BOTTLENECK_NUM: 0\n",
      "  DIM_FEEDFORWARD: 1024\n",
      "  DIM_MODEL: 256\n",
      "  ENCODER_LAYERS: 4\n",
      "  EXTRA:\n",
      "    DECONV_WITH_BIAS: False\n",
      "    FINAL_CONV_KERNEL: 1\n",
      "    NUM_DECONV_FILTERS: [256]\n",
      "    NUM_DECONV_KERNELS: [4]\n",
      "    NUM_DECONV_LAYERS: 1\n",
      "    NUM_LAYERS: 50\n",
      "  HEATMAP_SIZE: [48, 64]\n",
      "  IMAGE_SIZE: [192, 256]\n",
      "  INIT_WEIGHTS: True\n",
      "  INTERMEDIATE_SUP: False\n",
      "  NAME: transpose_r\n",
      "  NUM_JOINTS: 17\n",
      "  N_HEAD: 8\n",
      "  PE_ONLY_AT_BEGIN: False\n",
      "  POS_EMBEDDING: sine\n",
      "  PRETRAINED: models/pytorch/imagenet/resnet50-19c8e357.pth\n",
      "  SIGMA: 2\n",
      "  TAG_PER_JOINT: True\n",
      "  TARGET_TYPE: gaussian\n",
      "OUTPUT_DIR: output\n",
      "PIN_MEMORY: True\n",
      "PRINT_FREQ: 100\n",
      "RANK: 0\n",
      "TEST:\n",
      "  BATCH_SIZE_PER_GPU: 8\n",
      "  BBOX_THRE: 1.0\n",
      "  BLUR_KERNEL: 11\n",
      "  COCO_BBOX_FILE: data/coco/person_detection_results/COCO_val2017_detections_AP_H_56_person.json\n",
      "  FLIP_TEST: True\n",
      "  IMAGE_THRE: 0.0\n",
      "  IN_VIS_THRE: 0.2\n",
      "  MODEL_FILE: models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      "  NMS_THRE: 1.0\n",
      "  OKS_THRE: 0.9\n",
      "  POST_PROCESS: True\n",
      "  SHIFT_HEATMAP: True\n",
      "  SOFT_NMS: False\n",
      "  USE_GT_BBOX: True\n",
      "TRAIN:\n",
      "  BATCH_SIZE_PER_GPU: 20\n",
      "  BEGIN_EPOCH: 0\n",
      "  CHECKPOINT: \n",
      "  END_EPOCH: 230\n",
      "  GAMMA1: 0.99\n",
      "  GAMMA2: 0.0\n",
      "  LR: 0.0001\n",
      "  LR_END: 1e-05\n",
      "  LR_FACTOR: 0.25\n",
      "  LR_STEP: [100, 150, 200, 220]\n",
      "  MOMENTUM: 0.9\n",
      "  NESTEROV: False\n",
      "  OPTIMIZER: adam\n",
      "  RESUME: False\n",
      "  SHUFFLE: True\n",
      "  WD: 0.0001\n",
      "WORKERS: 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> creating output/coco/transpose_r/TP_R_256x192_d256_h1024_enc4_mh8\n",
      "=> creating log/coco/transpose_r/TP_R_256x192_d256_h1024_enc4_mh8_2024-03-22-11-54\n"
     ]
    }
   ],
   "source": [
    "logger, final_output_dir, tb_log_dir = create_logger(cfg, args.cfg, 'valid')\n",
    "\n",
    "logger.info(pprint.pformat(args))\n",
    "logger.info(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dfb4627-932c-4ee7-867c-1e7a08fbdf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = cfg.CUDNN.BENCHMARK\n",
    "torch.backends.cudnn.deterministic = cfg.CUDNN.DETERMINISTIC\n",
    "torch.backends.cudnn.enabled = cfg.CUDNN.ENABLED\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "eac918d1-d62e-49ad-b066-2498a0757b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==> Add Sine PositionEmbedding~\n"
     ]
    }
   ],
   "source": [
    "model = eval('models.'+cfg.MODEL.NAME+'.get_pose_net')(\n",
    "    cfg, is_train=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "cb6313f1-6940-4100-b105-6323fe7435d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4a8afd-e72a-4c7e-b989-3565060f0950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "3180bf7a-e33d-439f-9dd6-00b66a4a9f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model weights from cfg.TEST.MODEL_FILE: models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n"
     ]
    }
   ],
   "source": [
    "if cfg.TEST.MODEL_FILE:\n",
    "    # logger.info('=> loading model from {}'.format(cfg.TEST.MODEL_FILE))\n",
    "    print(f\"loading model weights from cfg.TEST.MODEL_FILE: {cfg.TEST.MODEL_FILE}\")\n",
    "    ckpt_state_dict = torch.load(cfg.TEST.MODEL_FILE)\n",
    "    # print(ckpt_state_dict['pos_embedding'])  # FOR UNSeen Resolutions\n",
    "    # ckpt_state_dict.pop('pos_embedding') # FOR UNSeen Resolutions\n",
    "    model.load_state_dict(ckpt_state_dict, strict=True)   #  strict=False FOR UNSeen Resolutions\n",
    "else:\n",
    "    model_state_file = os.path.join(\n",
    "        final_output_dir, 'final_state.pth'\n",
    "    )\n",
    "    print(f\"loading model weights from final_output_dir/final_state.pth: {model_state_file}\")\n",
    "    # logger.info('=> loading model from {}'.format(model_state_file))\n",
    "    model.load_state_dict(torch.load(model_state_file))\n",
    "w, h = cfg.MODEL.IMAGE_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "f1a2b79c-2261-4d87-bda5-d593acc177a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=> classes: ['__background__', 'person']\n",
      "=> num_images: 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.33s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=> load 6352 samples\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.DataParallel(model, device_ids=cfg.GPUS).cuda()\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "criterion = JointsMSELoss(\n",
    "    use_target_weight=cfg.LOSS.USE_TARGET_WEIGHT\n",
    ").cuda()\n",
    "\n",
    "# Data loading code\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "valid_dataset = eval('dataset.'+cfg.DATASET.DATASET)(\n",
    "    cfg, cfg.DATASET.ROOT, cfg.DATASET.TEST_SET, False,\n",
    "    transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=cfg.TEST.BATCH_SIZE_PER_GPU*len(cfg.GPUS),\n",
    "    shuffle=False,\n",
    "    num_workers=cfg.WORKERS,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "7f01152c-d44f-4729-b8de-52e307cd6c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_sparsity(layer):\n",
    "    count_wts = layer.weight.data.numel()\n",
    "    count_nz_wts = torch.sum(layer.weight.data.abs().clone().gt(0).float())\n",
    "    sparsity = round(1.0 - float(count_nz_wts/count_wts), 3)\n",
    "    print(f\"weights total: {count_wts} \\t nonzero: {count_nz_wts} \\t sparsity: {sparsity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "ac0b4297-9730-40d8-a2c3-81600bc941ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_net_sparsity_layerwise(dl_net):\n",
    "    skip_layer_types = [torch.nn.modules.container.Sequential,\n",
    "                        models.transpose_r.TransPoseR,\n",
    "                        torch.nn.modules.pooling.AdaptiveAvgPool2d,\n",
    "                        torch.nn.modules.activation.ReLU,\n",
    "                        torch.nn.modules.pooling.MaxPool2d,\n",
    "                        models.transpose_r.Bottleneck,\n",
    "                        models.transpose_r.TransformerEncoder,\n",
    "                        torch.nn.modules.container.ModuleList,\n",
    "                        models.transpose_r.TransformerEncoderLayer,\n",
    "                        torch.nn.modules.activation.MultiheadAttention,\n",
    "                        torch.nn.modules.dropout.Dropout\n",
    "                       ]\n",
    "    l_i = 0\n",
    "    for l in dl_net.named_modules():\n",
    "        skip = False\n",
    "        l_i += 1\n",
    "        # print(type(l[1]))\n",
    "        if l_i > 1 and not 'relu' in l[0].lower() and not 'maxpool' in l[0]:\n",
    "            for skip_layer in skip_layer_types:\n",
    "                if isinstance(l[1], skip_layer):\n",
    "                    skip = True\n",
    "\n",
    "            if skip:\n",
    "                continue\n",
    "            # if  isinstance(l[1], torch.nn.modules.container.Sequential):\n",
    "            #     continue\n",
    "            # # if  isinstance(l[1], models.resnet.BasicBlock):\n",
    "            # #     continue\n",
    "            # if isinstance(l[1], models.transpose_r.TransPoseR):\n",
    "            #     continue\n",
    "            # if  isinstance(l[1], torch.nn.modules.pooling.AdaptiveAvgPool2d):\n",
    "            #     continue\n",
    "            # if isinstance(l[1], torch.nn.modules.activation.ReLU):\n",
    "            #     continue\n",
    "            # if isinstance(l[1], torch.nn.modules.pooling.MaxPool2d):\n",
    "            #     continue\n",
    "                \n",
    "            print(l_i, end=\"\\t\")\n",
    "            print(l[0], end=\"\\t\" )\n",
    "            # print(type(l[1]), \"\\t\")\n",
    "            layer_sparsity(l[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "ee2a0a8a-34f8-4dbb-b88c-24545b9853e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model(model, prune_ratios):\n",
    "    l_i = 0\n",
    "    for l in model.named_modules():\n",
    "        l_i += 1\n",
    "        if l[0] in prune_ratios.keys():\n",
    "            print(l_i, end=\"\\t\")\n",
    "            print(l[0], end=\"\\t\" )\n",
    "            print(type(l[1]), \"\\t\")\n",
    "            prune.ln_structured(l[1], 'weight', amount=prune_ratios[l[0]], dim=1, n=float('-inf'))\n",
    "            print(\"^^^\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "0fc74af3-1de5-4863-baf8-0157dfb84ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_ratios_set1 = {}\n",
    "\n",
    "prune_ratios_set1['module.layer1.0.conv1'] = 0\n",
    "prune_ratios_set1['module.layer1.0.conv2'] = 0\n",
    "prune_ratios_set1['module.layer1.0.conv3'] = 0\n",
    "\n",
    "prune_ratios_set1['module.layer1.1.conv1'] = 0\n",
    "prune_ratios_set1['module.layer1.1.conv2'] = 0\n",
    "prune_ratios_set1['module.layer1.1.conv3'] = 0\n",
    "\n",
    "prune_ratios_set1['module.layer1.2.conv1'] = 0\n",
    "prune_ratios_set1['module.layer1.2.conv2'] = 0\n",
    "prune_ratios_set1['module.layer1.2.conv3'] = 0\n",
    "\n",
    "prune_ratios_set1['module.layer2.1.conv1'] = 0.5\n",
    "prune_ratios_set1['module.layer2.1.conv2'] = 0.5\n",
    "prune_ratios_set1['module.layer2.1.conv3'] = 0.5\n",
    "\n",
    "prune_ratios_set1['module.layer2.2.conv1'] = 0.5\n",
    "prune_ratios_set1['module.layer2.2.conv2'] = 0.5\n",
    "prune_ratios_set1['module.layer2.2.conv3'] = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "9dbb9655-eec9-4af0-bb70-c5d5717907d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\tmodule.layer1.0.conv1\t<class 'torch.nn.modules.conv.Conv2d'> \t\n",
      "^^^\n",
      "11\tmodule.layer1.0.conv2\t<class 'torch.nn.modules.conv.Conv2d'> \t\n",
      "^^^\n",
      "13\tmodule.layer1.0.conv3\t<class 'torch.nn.modules.conv.Conv2d'> \t\n",
      "^^^\n",
      "20\tmodule.layer1.1.conv1\t<class 'torch.nn.modules.conv.Conv2d'> \t\n",
      "^^^\n",
      "22\tmodule.layer1.1.conv2\t<class 'torch.nn.modules.conv.Conv2d'> \t\n",
      "^^^\n",
      "24\tmodule.layer1.1.conv3\t<class 'torch.nn.modules.conv.Conv2d'> \t\n",
      "^^^\n",
      "28\tmodule.layer1.2.conv1\t<class 'torch.nn.modules.conv.Conv2d'> \t\n",
      "^^^\n",
      "30\tmodule.layer1.2.conv2\t<class 'torch.nn.modules.conv.Conv2d'> \t\n",
      "^^^\n",
      "32\tmodule.layer1.2.conv3\t<class 'torch.nn.modules.conv.Conv2d'> \t\n",
      "^^^\n",
      "48\tmodule.layer2.1.conv1\t<class 'torch.nn.modules.conv.Conv2d'> \t\n",
      "^^^\n",
      "50\tmodule.layer2.1.conv2\t<class 'torch.nn.modules.conv.Conv2d'> \t\n",
      "^^^\n",
      "52\tmodule.layer2.1.conv3\t<class 'torch.nn.modules.conv.Conv2d'> \t\n",
      "^^^\n",
      "56\tmodule.layer2.2.conv1\t<class 'torch.nn.modules.conv.Conv2d'> \t\n",
      "^^^\n",
      "58\tmodule.layer2.2.conv2\t<class 'torch.nn.modules.conv.Conv2d'> \t\n",
      "^^^\n",
      "60\tmodule.layer2.2.conv3\t<class 'torch.nn.modules.conv.Conv2d'> \t\n",
      "^^^\n"
     ]
    }
   ],
   "source": [
    "prune_model(model, prune_ratios_set1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "480f6fd5-fa2e-4348-ae24-55d4a8e9cfad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\tmodule.conv1\tweights total: 9408 \t nonzero: 9408.0 \t sparsity: 0.0\n",
      "4\tmodule.bn1\tweights total: 64 \t nonzero: 64.0 \t sparsity: 0.0\n",
      "9\tmodule.layer1.0.conv1\tweights total: 4096 \t nonzero: 4096.0 \t sparsity: 0.0\n",
      "10\tmodule.layer1.0.bn1\tweights total: 64 \t nonzero: 64.0 \t sparsity: 0.0\n",
      "11\tmodule.layer1.0.conv2\tweights total: 36864 \t nonzero: 36864.0 \t sparsity: 0.0\n",
      "12\tmodule.layer1.0.bn2\tweights total: 64 \t nonzero: 64.0 \t sparsity: 0.0\n",
      "13\tmodule.layer1.0.conv3\tweights total: 16384 \t nonzero: 16384.0 \t sparsity: 0.0\n",
      "14\tmodule.layer1.0.bn3\tweights total: 256 \t nonzero: 256.0 \t sparsity: 0.0\n",
      "17\tmodule.layer1.0.downsample.0\tweights total: 16384 \t nonzero: 16384.0 \t sparsity: 0.0\n",
      "18\tmodule.layer1.0.downsample.1\tweights total: 256 \t nonzero: 256.0 \t sparsity: 0.0\n",
      "20\tmodule.layer1.1.conv1\tweights total: 16384 \t nonzero: 16384.0 \t sparsity: 0.0\n",
      "21\tmodule.layer1.1.bn1\tweights total: 64 \t nonzero: 64.0 \t sparsity: 0.0\n",
      "22\tmodule.layer1.1.conv2\tweights total: 36864 \t nonzero: 36864.0 \t sparsity: 0.0\n",
      "23\tmodule.layer1.1.bn2\tweights total: 64 \t nonzero: 64.0 \t sparsity: 0.0\n",
      "24\tmodule.layer1.1.conv3\tweights total: 16384 \t nonzero: 16384.0 \t sparsity: 0.0\n",
      "25\tmodule.layer1.1.bn3\tweights total: 256 \t nonzero: 256.0 \t sparsity: 0.0\n",
      "28\tmodule.layer1.2.conv1\tweights total: 16384 \t nonzero: 16384.0 \t sparsity: 0.0\n",
      "29\tmodule.layer1.2.bn1\tweights total: 64 \t nonzero: 64.0 \t sparsity: 0.0\n",
      "30\tmodule.layer1.2.conv2\tweights total: 36864 \t nonzero: 36864.0 \t sparsity: 0.0\n",
      "31\tmodule.layer1.2.bn2\tweights total: 64 \t nonzero: 64.0 \t sparsity: 0.0\n",
      "32\tmodule.layer1.2.conv3\tweights total: 16384 \t nonzero: 16384.0 \t sparsity: 0.0\n",
      "33\tmodule.layer1.2.bn3\tweights total: 256 \t nonzero: 256.0 \t sparsity: 0.0\n",
      "37\tmodule.layer2.0.conv1\tweights total: 32768 \t nonzero: 32768.0 \t sparsity: 0.0\n",
      "38\tmodule.layer2.0.bn1\tweights total: 128 \t nonzero: 128.0 \t sparsity: 0.0\n",
      "39\tmodule.layer2.0.conv2\tweights total: 147456 \t nonzero: 147456.0 \t sparsity: 0.0\n",
      "40\tmodule.layer2.0.bn2\tweights total: 128 \t nonzero: 128.0 \t sparsity: 0.0\n",
      "41\tmodule.layer2.0.conv3\tweights total: 65536 \t nonzero: 65536.0 \t sparsity: 0.0\n",
      "42\tmodule.layer2.0.bn3\tweights total: 512 \t nonzero: 512.0 \t sparsity: 0.0\n",
      "45\tmodule.layer2.0.downsample.0\tweights total: 131072 \t nonzero: 131072.0 \t sparsity: 0.0\n",
      "46\tmodule.layer2.0.downsample.1\tweights total: 512 \t nonzero: 512.0 \t sparsity: 0.0\n",
      "48\tmodule.layer2.1.conv1\tweights total: 65536 \t nonzero: 32768.0 \t sparsity: 0.5\n",
      "49\tmodule.layer2.1.bn1\tweights total: 128 \t nonzero: 128.0 \t sparsity: 0.0\n",
      "50\tmodule.layer2.1.conv2\tweights total: 147456 \t nonzero: 73728.0 \t sparsity: 0.5\n",
      "51\tmodule.layer2.1.bn2\tweights total: 128 \t nonzero: 128.0 \t sparsity: 0.0\n",
      "52\tmodule.layer2.1.conv3\tweights total: 65536 \t nonzero: 32768.0 \t sparsity: 0.5\n",
      "53\tmodule.layer2.1.bn3\tweights total: 512 \t nonzero: 512.0 \t sparsity: 0.0\n",
      "56\tmodule.layer2.2.conv1\tweights total: 65536 \t nonzero: 32768.0 \t sparsity: 0.5\n",
      "57\tmodule.layer2.2.bn1\tweights total: 128 \t nonzero: 128.0 \t sparsity: 0.0\n",
      "58\tmodule.layer2.2.conv2\tweights total: 147456 \t nonzero: 73728.0 \t sparsity: 0.5\n",
      "59\tmodule.layer2.2.bn2\tweights total: 128 \t nonzero: 128.0 \t sparsity: 0.0\n",
      "60\tmodule.layer2.2.conv3\tweights total: 65536 \t nonzero: 32768.0 \t sparsity: 0.5\n",
      "61\tmodule.layer2.2.bn3\tweights total: 512 \t nonzero: 512.0 \t sparsity: 0.0\n",
      "64\tmodule.layer2.3.conv1\tweights total: 65536 \t nonzero: 65536.0 \t sparsity: 0.0\n",
      "65\tmodule.layer2.3.bn1\tweights total: 128 \t nonzero: 128.0 \t sparsity: 0.0\n",
      "66\tmodule.layer2.3.conv2\tweights total: 147456 \t nonzero: 147456.0 \t sparsity: 0.0\n",
      "67\tmodule.layer2.3.bn2\tweights total: 128 \t nonzero: 128.0 \t sparsity: 0.0\n",
      "68\tmodule.layer2.3.conv3\tweights total: 65536 \t nonzero: 65536.0 \t sparsity: 0.0\n",
      "69\tmodule.layer2.3.bn3\tweights total: 512 \t nonzero: 512.0 \t sparsity: 0.0\n",
      "71\tmodule.reduce\tweights total: 131072 \t nonzero: 131072.0 \t sparsity: 0.0\n",
      "76\tmodule.global_encoder.layers.0.self_attn.out_proj\tweights total: 65536 \t nonzero: 65536.0 \t sparsity: 0.0\n",
      "77\tmodule.global_encoder.layers.0.linear1\tweights total: 262144 \t nonzero: 262144.0 \t sparsity: 0.0\n",
      "79\tmodule.global_encoder.layers.0.linear2\tweights total: 262144 \t nonzero: 262144.0 \t sparsity: 0.0\n",
      "80\tmodule.global_encoder.layers.0.norm1\tweights total: 256 \t nonzero: 256.0 \t sparsity: 0.0\n",
      "81\tmodule.global_encoder.layers.0.norm2\tweights total: 256 \t nonzero: 256.0 \t sparsity: 0.0\n",
      "86\tmodule.global_encoder.layers.1.self_attn.out_proj\tweights total: 65536 \t nonzero: 65536.0 \t sparsity: 0.0\n",
      "87\tmodule.global_encoder.layers.1.linear1\tweights total: 262144 \t nonzero: 262144.0 \t sparsity: 0.0\n",
      "89\tmodule.global_encoder.layers.1.linear2\tweights total: 262144 \t nonzero: 262144.0 \t sparsity: 0.0\n",
      "90\tmodule.global_encoder.layers.1.norm1\tweights total: 256 \t nonzero: 256.0 \t sparsity: 0.0\n",
      "91\tmodule.global_encoder.layers.1.norm2\tweights total: 256 \t nonzero: 256.0 \t sparsity: 0.0\n",
      "96\tmodule.global_encoder.layers.2.self_attn.out_proj\tweights total: 65536 \t nonzero: 65536.0 \t sparsity: 0.0\n",
      "97\tmodule.global_encoder.layers.2.linear1\tweights total: 262144 \t nonzero: 262144.0 \t sparsity: 0.0\n",
      "99\tmodule.global_encoder.layers.2.linear2\tweights total: 262144 \t nonzero: 262144.0 \t sparsity: 0.0\n",
      "100\tmodule.global_encoder.layers.2.norm1\tweights total: 256 \t nonzero: 256.0 \t sparsity: 0.0\n",
      "101\tmodule.global_encoder.layers.2.norm2\tweights total: 256 \t nonzero: 256.0 \t sparsity: 0.0\n",
      "106\tmodule.global_encoder.layers.3.self_attn.out_proj\tweights total: 65536 \t nonzero: 65536.0 \t sparsity: 0.0\n",
      "107\tmodule.global_encoder.layers.3.linear1\tweights total: 262144 \t nonzero: 262144.0 \t sparsity: 0.0\n",
      "109\tmodule.global_encoder.layers.3.linear2\tweights total: 262144 \t nonzero: 262144.0 \t sparsity: 0.0\n",
      "110\tmodule.global_encoder.layers.3.norm1\tweights total: 256 \t nonzero: 256.0 \t sparsity: 0.0\n",
      "111\tmodule.global_encoder.layers.3.norm2\tweights total: 256 \t nonzero: 256.0 \t sparsity: 0.0\n",
      "115\tmodule.deconv_layers.0\tweights total: 1048576 \t nonzero: 1048576.0 \t sparsity: 0.0\n",
      "116\tmodule.deconv_layers.1\tweights total: 256 \t nonzero: 256.0 \t sparsity: 0.0\n",
      "118\tmodule.final_layer\tweights total: 4352 \t nonzero: 4352.0 \t sparsity: 0.0\n"
     ]
    }
   ],
   "source": [
    "print_net_sparsity_layerwise(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "d9efae91-84cf-4ab6-9101-2aee216e5144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: [0/794]\tTime 1.288 (1.288)\tLoss 0.0009 (0.0009)\tAccuracy 0.607 (0.607)\n",
      "Test: [100/794]\tTime 0.230 (0.242)\tLoss 0.0011 (0.0010)\tAccuracy 0.484 (0.358)\n",
      "Test: [200/794]\tTime 0.229 (0.235)\tLoss 0.0014 (0.0010)\tAccuracy 0.227 (0.346)\n",
      "Test: [300/794]\tTime 0.231 (0.233)\tLoss 0.0014 (0.0010)\tAccuracy 0.268 (0.338)\n",
      "Test: [400/794]\tTime 0.236 (0.233)\tLoss 0.0007 (0.0010)\tAccuracy 0.584 (0.339)\n",
      "Test: [500/794]\tTime 0.235 (0.233)\tLoss 0.0013 (0.0010)\tAccuracy 0.438 (0.337)\n",
      "Test: [600/794]\tTime 0.248 (0.234)\tLoss 0.0012 (0.0010)\tAccuracy 0.480 (0.336)\n",
      "Test: [700/794]\tTime 0.256 (0.235)\tLoss 0.0013 (0.0010)\tAccuracy 0.339 (0.336)\n",
      "=> writing results json to output/coco/transpose_r/TP_R_256x192_d256_h1024_enc4_mh8/results/keypoints_val2017_results_0.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.42s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *keypoints*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Arch | AP | Ap .5 | AP .75 | AP (M) | AP (L) | AR | AR .5 | AR .75 | AR (M) | AR (L) |\n",
      "|---|---|---|---|---|---|---|---|---|---|---|\n",
      "| transpose_r | 0.082 | 0.202 | 0.054 | 0.060 | 0.114 | 0.111 | 0.252 | 0.087 | 0.083 | 0.151 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE (t=2.50s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.06s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.082\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.202\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.054\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.060\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.114\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.111\n",
      " Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.252\n",
      " Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.087\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.083\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.08215065935656146"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(cfg, valid_loader, valid_dataset, model, criterion,\n",
    "             final_output_dir, tb_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bbb4ca-c47a-4e47-99c8-f7e399e62b38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80595f7-d517-408a-a54c-88c81d5a77ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
