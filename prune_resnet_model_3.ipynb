{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8f721ec-b3cf-4e56-b34e-6a7b179b596f",
   "metadata": {},
   "source": [
    "## What does this notebook do?\n",
    "\n",
    "This notebook compares the original TransPose model with the same model after pruning for 2 iterations\n",
    "- A chart is made which shows num of weights in the conv layers in original model and pruned models.\n",
    "- This helps to visualise the size impact of pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b27e3be-4e12-42d8-ac5b-26e92b3637b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "from torch import nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import tptools._init_paths\n",
    "from config import cfg\n",
    "from config import update_config\n",
    "from core.loss import JointsMSELoss\n",
    "from core.function import validate\n",
    "from utils.utils import create_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3c4aba8-0047-43a0-a66a-5729878e2603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aec49e73-f5ab-4f13-b1f3-995fb11ac639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset\n",
    "import models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6f40f72-8314-4ed9-bb45-f217a0807184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9e79c4f-32e7-458d-904c-2cbe59ae73f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca6301b4-3e6a-4658-94b1-8843eadc34df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(args___):\n",
    "    parser = argparse.ArgumentParser(description='Train keypoints network')\n",
    "    # general\n",
    "    parser.add_argument('--cfg',\n",
    "                        help='experiment configure file name',\n",
    "                        required=True,\n",
    "                        type=str)\n",
    "\n",
    "    parser.add_argument('opts',\n",
    "                        help=\"Modify config options using the command-line\",\n",
    "                        default=None,\n",
    "                        nargs=argparse.REMAINDER)\n",
    "\n",
    "    parser.add_argument('--modelDir',\n",
    "                        help='model directory',\n",
    "                        type=str,\n",
    "                        default='')\n",
    "    parser.add_argument('--logDir',\n",
    "                        help='log directory',\n",
    "                        type=str,\n",
    "                        default='')\n",
    "    parser.add_argument('--dataDir',\n",
    "                        help='data directory',\n",
    "                        type=str,\n",
    "                        default='')\n",
    "    parser.add_argument('--prevModelDir',\n",
    "                        help='prev Model directory',\n",
    "                        type=str,\n",
    "                        default='')\n",
    "\n",
    "    args = parser.parse_args(args___)\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d13c660-f5b1-462e-b6bf-e82765b1b096",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = 'experiments/coco/transpose_r/TP_R_256x192_d256_h1024_enc4_mh8_chsel.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9033446a-f585-4733-8018-4a7ab71b51ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_file_path = 'experiments/coco/transpose_h/TP_H_w32_256x192_stage3_1_4_d64_h128_relu_enc4_mh1.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3edce75-80ec-45d9-b31d-d6f159577411",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args(['--cfg', config_file_path, 'TEST.USE_GT_BBOX', 'True'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bc8e5be-abb2-40a3-add0-637d16788e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_config(cfg, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fcd3587-9ebe-4662-91ae-c2fe515f0caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CfgNode({'OUTPUT_DIR': 'output', 'LOG_DIR': 'log', 'DATA_DIR': '', 'GPUS': (0,), 'WORKERS': 1, 'PRINT_FREQ': 100, 'AUTO_RESUME': True, 'PIN_MEMORY': True, 'RANK': 0, 'CUDNN': CfgNode({'BENCHMARK': True, 'DETERMINISTIC': False, 'ENABLED': True}), 'MODEL': CfgNode({'NAME': 'transpose_r_chsel', 'INIT_WEIGHTS': True, 'PRETRAINED': 'models/pytorch/imagenet/resnet50-19c8e357.pth', 'PRETRAINED_CHSEL': 'output/coco/transpose_r_chsel/TP_R_256x192_d256_h1024_enc4_mh8_chsel/pretrained_chsel_1.pth', 'NUM_JOINTS': 17, 'TAG_PER_JOINT': True, 'TARGET_TYPE': 'gaussian', 'IMAGE_SIZE': [192, 256], 'HEATMAP_SIZE': [48, 64], 'SIGMA': 2, 'EXTRA': CfgNode({'FINAL_CONV_KERNEL': 1, 'DECONV_WITH_BIAS': False, 'NUM_DECONV_LAYERS': 1, 'NUM_DECONV_FILTERS': [256], 'NUM_DECONV_KERNELS': [4], 'NUM_LAYERS': 50}), 'NW_CFG': None, 'BOTTLENECK_NUM': 0, 'DIM_MODEL': 256, 'DIM_FEEDFORWARD': 1024, 'ENCODER_LAYERS': 4, 'N_HEAD': 8, 'ATTENTION_ACTIVATION': 'relu', 'POS_EMBEDDING': 'sine', 'INTERMEDIATE_SUP': False, 'PE_ONLY_AT_BEGIN': False}), 'LOSS': CfgNode({'USE_OHKM': False, 'TOPK': 8, 'USE_TARGET_WEIGHT': True, 'USE_DIFFERENT_JOINTS_WEIGHT': False}), 'DATASET': CfgNode({'ROOT': 'data/coco/', 'DATASET': 'coco', 'TRAIN_SET': 'train2017', 'TEST_SET': 'val2017', 'DATA_FORMAT': 'jpg', 'HYBRID_JOINTS_TYPE': '', 'SELECT_DATA': False, 'FLIP': True, 'SCALE_FACTOR': 0.35, 'ROT_FACTOR': 45, 'PROB_HALF_BODY': 0.3, 'NUM_JOINTS_HALF_BODY': 8, 'COLOR_RGB': True}), 'TRAIN': CfgNode({'LR_FACTOR': 0.25, 'LR_STEP': [100, 150, 200, 220], 'LR': 0.0001, 'LR_END': 1e-05, 'OPTIMIZER': 'adam', 'MOMENTUM': 0.9, 'WD': 0.0001, 'NESTEROV': False, 'GAMMA1': 0.99, 'GAMMA2': 0.0, 'BEGIN_EPOCH': 0, 'END_EPOCH': 230, 'RESUME': False, 'CHECKPOINT': '', 'BATCH_SIZE_PER_GPU': 8, 'SHUFFLE': True}), 'TEST': CfgNode({'BLUR_KERNEL': 11, 'BATCH_SIZE_PER_GPU': 8, 'FLIP_TEST': True, 'POST_PROCESS': True, 'SHIFT_HEATMAP': True, 'USE_GT_BBOX': True, 'IMAGE_THRE': 0.0, 'NMS_THRE': 1.0, 'SOFT_NMS': False, 'OKS_THRE': 0.9, 'IN_VIS_THRE': 0.2, 'COCO_BBOX_FILE': 'data/coco/person_detection_results/COCO_val2017_detections_AP_H_56_person.json', 'BBOX_THRE': 1.0, 'MODEL_FILE': 'models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth'}), 'DEBUG': CfgNode({'DEBUG': False, 'SAVE_BATCH_IMAGES_GT': True, 'SAVE_BATCH_IMAGES_PRED': True, 'SAVE_HEATMAPS_GT': True, 'SAVE_HEATMAPS_PRED': True})})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a51846db-e07c-4a6d-b7a0-2f4daaddb02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Namespace(cfg='experiments/coco/transpose_r/TP_R_256x192_d256_h1024_enc4_mh8_chsel.yaml', opts=['TEST.USE_GT_BBOX', 'True'], modelDir='', logDir='', dataDir='', prevModelDir='')\n",
      "AUTO_RESUME: True\n",
      "CUDNN:\n",
      "  BENCHMARK: True\n",
      "  DETERMINISTIC: False\n",
      "  ENABLED: True\n",
      "DATASET:\n",
      "  COLOR_RGB: True\n",
      "  DATASET: coco\n",
      "  DATA_FORMAT: jpg\n",
      "  FLIP: True\n",
      "  HYBRID_JOINTS_TYPE: \n",
      "  NUM_JOINTS_HALF_BODY: 8\n",
      "  PROB_HALF_BODY: 0.3\n",
      "  ROOT: data/coco/\n",
      "  ROT_FACTOR: 45\n",
      "  SCALE_FACTOR: 0.35\n",
      "  SELECT_DATA: False\n",
      "  TEST_SET: val2017\n",
      "  TRAIN_SET: train2017\n",
      "DATA_DIR: \n",
      "DEBUG:\n",
      "  DEBUG: False\n",
      "  SAVE_BATCH_IMAGES_GT: True\n",
      "  SAVE_BATCH_IMAGES_PRED: True\n",
      "  SAVE_HEATMAPS_GT: True\n",
      "  SAVE_HEATMAPS_PRED: True\n",
      "GPUS: (0,)\n",
      "LOG_DIR: log\n",
      "LOSS:\n",
      "  TOPK: 8\n",
      "  USE_DIFFERENT_JOINTS_WEIGHT: False\n",
      "  USE_OHKM: False\n",
      "  USE_TARGET_WEIGHT: True\n",
      "MODEL:\n",
      "  ATTENTION_ACTIVATION: relu\n",
      "  BOTTLENECK_NUM: 0\n",
      "  DIM_FEEDFORWARD: 1024\n",
      "  DIM_MODEL: 256\n",
      "  ENCODER_LAYERS: 4\n",
      "  EXTRA:\n",
      "    DECONV_WITH_BIAS: False\n",
      "    FINAL_CONV_KERNEL: 1\n",
      "    NUM_DECONV_FILTERS: [256]\n",
      "    NUM_DECONV_KERNELS: [4]\n",
      "    NUM_DECONV_LAYERS: 1\n",
      "    NUM_LAYERS: 50\n",
      "  HEATMAP_SIZE: [48, 64]\n",
      "  IMAGE_SIZE: [192, 256]\n",
      "  INIT_WEIGHTS: True\n",
      "  INTERMEDIATE_SUP: False\n",
      "  NAME: transpose_r_chsel\n",
      "  NUM_JOINTS: 17\n",
      "  NW_CFG: None\n",
      "  N_HEAD: 8\n",
      "  PE_ONLY_AT_BEGIN: False\n",
      "  POS_EMBEDDING: sine\n",
      "  PRETRAINED: models/pytorch/imagenet/resnet50-19c8e357.pth\n",
      "  PRETRAINED_CHSEL: output/coco/transpose_r_chsel/TP_R_256x192_d256_h1024_enc4_mh8_chsel/pretrained_chsel_1.pth\n",
      "  SIGMA: 2\n",
      "  TAG_PER_JOINT: True\n",
      "  TARGET_TYPE: gaussian\n",
      "OUTPUT_DIR: output\n",
      "PIN_MEMORY: True\n",
      "PRINT_FREQ: 100\n",
      "RANK: 0\n",
      "TEST:\n",
      "  BATCH_SIZE_PER_GPU: 8\n",
      "  BBOX_THRE: 1.0\n",
      "  BLUR_KERNEL: 11\n",
      "  COCO_BBOX_FILE: data/coco/person_detection_results/COCO_val2017_detections_AP_H_56_person.json\n",
      "  FLIP_TEST: True\n",
      "  IMAGE_THRE: 0.0\n",
      "  IN_VIS_THRE: 0.2\n",
      "  MODEL_FILE: models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      "  NMS_THRE: 1.0\n",
      "  OKS_THRE: 0.9\n",
      "  POST_PROCESS: True\n",
      "  SHIFT_HEATMAP: True\n",
      "  SOFT_NMS: False\n",
      "  USE_GT_BBOX: True\n",
      "TRAIN:\n",
      "  BATCH_SIZE_PER_GPU: 8\n",
      "  BEGIN_EPOCH: 0\n",
      "  CHECKPOINT: \n",
      "  END_EPOCH: 230\n",
      "  GAMMA1: 0.99\n",
      "  GAMMA2: 0.0\n",
      "  LR: 0.0001\n",
      "  LR_END: 1e-05\n",
      "  LR_FACTOR: 0.25\n",
      "  LR_STEP: [100, 150, 200, 220]\n",
      "  MOMENTUM: 0.9\n",
      "  NESTEROV: False\n",
      "  OPTIMIZER: adam\n",
      "  RESUME: False\n",
      "  SHUFFLE: True\n",
      "  WD: 0.0001\n",
      "WORKERS: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> creating output/coco/transpose_r_chsel/TP_R_256x192_d256_h1024_enc4_mh8_chsel\n",
      "=> creating log/coco/transpose_r_chsel/TP_R_256x192_d256_h1024_enc4_mh8_chsel_2024-06-12-14-21\n"
     ]
    }
   ],
   "source": [
    "logger, final_output_dir, tb_log_dir = create_logger(cfg, args.cfg, 'valid')\n",
    "\n",
    "logger.info(pprint.pformat(args))\n",
    "logger.info(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dfb4627-932c-4ee7-867c-1e7a08fbdf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = cfg.CUDNN.BENCHMARK\n",
    "torch.backends.cudnn.deterministic = cfg.CUDNN.DETERMINISTIC\n",
    "torch.backends.cudnn.enabled = cfg.CUDNN.ENABLED\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eac918d1-d62e-49ad-b066-2498a0757b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==> Add Sine PositionEmbedding~\n"
     ]
    }
   ],
   "source": [
    "model = eval('models.'+cfg.MODEL.NAME+'.get_pose_net')(\n",
    "    cfg, is_train=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb6313f1-6940-4100-b105-6323fe7435d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=> init final conv weights from normal distribution\n",
      "=> init .weight as normal(0, 0.001)\n",
      "=> init .bias as 0\n",
      "=> loading pretrained model models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: pos_embedding is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: conv1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: bn1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: bn1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: bn1.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: bn1.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: bn1.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.conv1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.bn1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.bn1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.bn1.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.bn1.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.bn1.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.conv2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.bn2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.bn2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.bn2.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.bn2.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.bn2.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.conv3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.bn3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.bn3.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.bn3.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.bn3.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.bn3.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.downsample.0.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.downsample.1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.downsample.1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.downsample.1.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.downsample.1.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.0.downsample.1.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.1.conv1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.1.bn1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.1.bn1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.1.bn1.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.1.bn1.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.1.bn1.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.1.conv2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.1.bn2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.1.bn2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.1.bn2.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.1.bn2.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.1.bn2.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.1.conv3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.1.bn3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.1.bn3.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.1.bn3.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.1.bn3.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.1.bn3.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.2.conv1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.2.bn1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.2.bn1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.2.bn1.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.2.bn1.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.2.bn1.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.2.conv2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.2.bn2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.2.bn2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.2.bn2.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.2.bn2.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.2.bn2.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.2.conv3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.2.bn3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.2.bn3.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.2.bn3.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.2.bn3.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer1.2.bn3.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.conv1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.bn1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.bn1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.bn1.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.bn1.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.bn1.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.conv2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.bn2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.bn2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.bn2.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.bn2.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.bn2.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.conv3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.bn3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.bn3.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.bn3.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.bn3.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.bn3.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.downsample.0.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.downsample.1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.downsample.1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.downsample.1.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.downsample.1.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.0.downsample.1.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.1.conv1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.1.bn1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.1.bn1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.1.bn1.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.1.bn1.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.1.bn1.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.1.conv2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.1.bn2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.1.bn2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.1.bn2.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.1.bn2.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.1.bn2.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.1.conv3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.1.bn3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.1.bn3.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.1.bn3.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.1.bn3.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.1.bn3.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.2.conv1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.2.bn1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.2.bn1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.2.bn1.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.2.bn1.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.2.bn1.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.2.conv2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.2.bn2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.2.bn2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.2.bn2.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.2.bn2.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.2.bn2.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.2.conv3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.2.bn3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.2.bn3.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.2.bn3.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.2.bn3.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.2.bn3.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.3.conv1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.3.bn1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.3.bn1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.3.bn1.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.3.bn1.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.3.bn1.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.3.conv2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.3.bn2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.3.bn2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.3.bn2.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.3.bn2.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.3.bn2.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.3.conv3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.3.bn3.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.3.bn3.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.3.bn3.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.3.bn3.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: layer2.3.bn3.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: reduce.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.0.self_attn.in_proj_weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.0.self_attn.in_proj_bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.0.self_attn.out_proj.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.0.self_attn.out_proj.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.0.linear1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.0.linear1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.0.linear2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.0.linear2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.0.norm1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.0.norm1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.0.norm2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.0.norm2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.1.self_attn.in_proj_weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.1.self_attn.in_proj_bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.1.self_attn.out_proj.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.1.self_attn.out_proj.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.1.linear1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.1.linear1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.1.linear2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.1.linear2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.1.norm1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.1.norm1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.1.norm2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.1.norm2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.2.self_attn.in_proj_weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.2.self_attn.in_proj_bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.2.self_attn.out_proj.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.2.self_attn.out_proj.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.2.linear1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.2.linear1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.2.linear2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.2.linear2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.2.norm1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.2.norm1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.2.norm2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.2.norm2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.3.self_attn.in_proj_weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.3.self_attn.in_proj_bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.3.self_attn.out_proj.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.3.self_attn.out_proj.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.3.linear1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.3.linear1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.3.linear2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.3.linear2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.3.norm1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.3.norm1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.3.norm2.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: global_encoder.layers.3.norm2.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: deconv_layers.0.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: deconv_layers.1.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: deconv_layers.1.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: deconv_layers.1.running_mean is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: deconv_layers.1.running_var is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: deconv_layers.1.num_batches_tracked is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: final_layer.weight is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n",
      ":: final_layer.bias is loaded from models/pytorch/transpose_coco/tp_r_256x192_enc4_d256_h1024_mh8.pth\n"
     ]
    }
   ],
   "source": [
    "model.init_weights(cfg.TEST.MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4a8afd-e72a-4c7e-b989-3565060f0950",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3180bf7a-e33d-439f-9dd6-00b66a4a9f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if cfg.TEST.MODEL_FILE:\n",
    "#     # logger.info('=> loading model from {}'.format(cfg.TEST.MODEL_FILE))\n",
    "#     print(f\"loading model weights from cfg.TEST.MODEL_FILE: {cfg.TEST.MODEL_FILE}\")\n",
    "#     ckpt_state_dict = torch.load(cfg.TEST.MODEL_FILE)\n",
    "#     # print(ckpt_state_dict['pos_embedding'])  # FOR UNSeen Resolutions\n",
    "#     # ckpt_state_dict.pop('pos_embedding') # FOR UNSeen Resolutions\n",
    "#     model.load_state_dict(ckpt_state_dict, strict=True)   #  strict=False FOR UNSeen Resolutions\n",
    "# else:\n",
    "#     model_state_file = os.path.join(\n",
    "#         final_output_dir, 'final_state.pth'\n",
    "#     )\n",
    "#     print(f\"loading model weights from final_output_dir/final_state.pth: {model_state_file}\")\n",
    "#     # logger.info('=> loading model from {}'.format(model_state_file))\n",
    "#     model.load_state_dict(torch.load(model_state_file))\n",
    "# w, h = cfg.MODEL.IMAGE_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1a2b79c-2261-4d87-bda5-d593acc177a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=> classes: ['__background__', 'person']\n",
      "=> num_images: 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.21s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=> load 6352 samples\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.DataParallel(model, device_ids=cfg.GPUS).cuda()\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "criterion = JointsMSELoss(\n",
    "    use_target_weight=cfg.LOSS.USE_TARGET_WEIGHT\n",
    ").cuda()\n",
    "\n",
    "# Data loading code\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "valid_dataset = eval('dataset.'+cfg.DATASET.DATASET)(\n",
    "    cfg, cfg.DATASET.ROOT, cfg.DATASET.TEST_SET, False,\n",
    "    transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=cfg.TEST.BATCH_SIZE_PER_GPU*len(cfg.GPUS),\n",
    "    shuffle=False,\n",
    "    num_workers=cfg.WORKERS,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1a94f30-6b7c-423a-bf73-2ac2be677fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nw_cfg_file = os.path.join(final_output_dir, \"transpose_r_pruned_iter1_nw_cfg.txt\")\n",
    "with open(nw_cfg_file, \"r+\") as nw_cfg_fd:\n",
    "    nw_cfg_str = nw_cfg_fd.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0455fa1a-ce4e-469b-86df-bcb9038483a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31, 24, 32, 39, 38, 60, 110, 102, 22, 61, 68, 92, 60, 105]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "nw_cfg = ast.literal_eval(nw_cfg_str)\n",
    "nw_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330b554a-e9df-40e8-9529-192a2f273fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0053fb7-d2ab-41d5-8cb8-9cbe3b974ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = \"models/pytorch/\"\n",
    "model_finetune1_path = \"transpose_r_modelfile_finetune_iter1.pth\"\n",
    "nw_cfg_1 = [31, 24, 32, 39, 38, 60, 110, 102, 22, 61, 68, 92, 60, 105]\n",
    "model_finetune2_path = \"transpose_r_modelfile_finetune_iter2.pth\"\n",
    "nw_cfg_2 = [30, 17, 24, 28, 35, 57, 99, 86, 14, 46, 55, 78, 53, 90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1657d2b6-d461-41b5-a2a7-8cac4b55b737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==> Add Sine PositionEmbedding~\n"
     ]
    }
   ],
   "source": [
    "pruned_model = eval('models.'+cfg.MODEL.NAME+'.get_pose_net')(\n",
    "    cfg, is_train=False, nw_cfg = nw_cfg_1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6711f9d5-54e2-458e-9e12-cc474dd87863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=> init final conv weights from normal distribution\n",
      "=> init .weight as normal(0, 0.001)\n",
      "=> init .bias as 0\n",
      "=> loading pretrained model models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: pos_embedding is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: conv1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: bn1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: bn1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: bn1.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: bn1.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: bn1.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.0.conv1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.0.bn1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.0.bn1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.0.bn1.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.0.bn1.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.0.bn1.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.0.select.indexes is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.0.conv2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.0.bn2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.0.bn2.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.0.bn2.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.0.bn2.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.0.bn2.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.0.conv3.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.0.bn3.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.0.bn3.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.0.bn3.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.0.bn3.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.0.bn3.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.0.downsample.0.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.0.downsample.1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.0.downsample.1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.0.downsample.1.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.0.downsample.1.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.0.downsample.1.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.1.conv1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.1.bn1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.1.bn1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.1.bn1.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.1.bn1.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.1.bn1.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.1.select.indexes is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.1.conv2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.1.bn2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.1.bn2.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.1.bn2.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.1.bn2.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.1.bn2.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.1.conv3.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.1.bn3.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.1.bn3.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.1.bn3.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.1.bn3.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.1.bn3.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.2.conv1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.2.bn1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.2.bn1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.2.bn1.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.2.bn1.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.2.bn1.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.2.select.indexes is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.2.conv2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.2.bn2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.2.bn2.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.2.bn2.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.2.bn2.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.2.bn2.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.2.conv3.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.2.bn3.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.2.bn3.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.2.bn3.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.2.bn3.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer1.2.bn3.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.0.conv1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.0.bn1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.0.bn1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.0.bn1.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.0.bn1.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.0.bn1.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.0.select.indexes is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.0.conv2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.0.bn2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.0.bn2.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.0.bn2.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.0.bn2.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.0.bn2.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.0.conv3.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.0.bn3.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.0.bn3.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.0.bn3.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.0.bn3.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.0.bn3.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.0.downsample.0.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.0.downsample.1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.0.downsample.1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.0.downsample.1.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.0.downsample.1.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.0.downsample.1.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.1.conv1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.1.bn1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.1.bn1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.1.bn1.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.1.bn1.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.1.bn1.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.1.select.indexes is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.1.conv2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.1.bn2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.1.bn2.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.1.bn2.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.1.bn2.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.1.bn2.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.1.conv3.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.1.bn3.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.1.bn3.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.1.bn3.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.1.bn3.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.1.bn3.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.2.conv1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.2.bn1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.2.bn1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.2.bn1.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.2.bn1.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.2.bn1.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.2.select.indexes is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.2.conv2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.2.bn2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.2.bn2.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.2.bn2.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.2.bn2.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.2.bn2.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.2.conv3.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.2.bn3.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.2.bn3.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.2.bn3.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.2.bn3.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.2.bn3.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.3.conv1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.3.bn1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.3.bn1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.3.bn1.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.3.bn1.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.3.bn1.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.3.select.indexes is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.3.conv2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.3.bn2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.3.bn2.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.3.bn2.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.3.bn2.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.3.bn2.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.3.conv3.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.3.bn3.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.3.bn3.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.3.bn3.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.3.bn3.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: layer2.3.bn3.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: reduce.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.0.self_attn.in_proj_weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.0.self_attn.in_proj_bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.0.self_attn.out_proj.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.0.self_attn.out_proj.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.0.linear1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.0.linear1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.0.linear2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.0.linear2.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.0.norm1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.0.norm1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.0.norm2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.0.norm2.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.1.self_attn.in_proj_weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.1.self_attn.in_proj_bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.1.self_attn.out_proj.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.1.self_attn.out_proj.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.1.linear1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.1.linear1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.1.linear2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.1.linear2.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.1.norm1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.1.norm1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.1.norm2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.1.norm2.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.2.self_attn.in_proj_weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.2.self_attn.in_proj_bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.2.self_attn.out_proj.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.2.self_attn.out_proj.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.2.linear1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.2.linear1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.2.linear2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.2.linear2.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.2.norm1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.2.norm1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.2.norm2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.2.norm2.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.3.self_attn.in_proj_weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.3.self_attn.in_proj_bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.3.self_attn.out_proj.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.3.self_attn.out_proj.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.3.linear1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.3.linear1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.3.linear2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.3.linear2.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.3.norm1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.3.norm1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.3.norm2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: global_encoder.layers.3.norm2.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: deconv_layers.0.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: deconv_layers.1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: deconv_layers.1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: deconv_layers.1.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: deconv_layers.1.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: deconv_layers.1.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: final_layer.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n",
      ":: final_layer.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter1.pth\n"
     ]
    }
   ],
   "source": [
    "pruned_model.init_weights(os.path.join(models_dir, model_finetune1_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5bd5c3ce-2c29-4876-aa4f-f272de9e3f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, conv1, conv1,\t\t\t, 9408, 9408, 1.0\n",
      "7, layer1.0.conv1, layer1.0.conv1,\t\t\t, 4096, 4096, 1.0\n",
      "10, layer1.0.conv2, layer1.0.conv2,\t\t\t, 36864, 6696, 0.181640625\n",
      "12, layer1.0.conv3, layer1.0.conv3,\t\t\t, 16384, 6144, 0.375\n",
      "16, layer1.0.downsample.0, layer1.0.downsample.0,\t\t\t, 16384, 16384, 1.0\n",
      "19, layer1.1.conv1, layer1.1.conv1,\t\t\t, 16384, 16384, 1.0\n",
      "22, layer1.1.conv2, layer1.1.conv2,\t\t\t, 36864, 11232, 0.3046875\n",
      "24, layer1.1.conv3, layer1.1.conv3,\t\t\t, 16384, 9984, 0.609375\n",
      "28, layer1.2.conv1, layer1.2.conv1,\t\t\t, 16384, 16384, 1.0\n",
      "31, layer1.2.conv2, layer1.2.conv2,\t\t\t, 36864, 20520, 0.556640625\n",
      "33, layer1.2.conv3, layer1.2.conv3,\t\t\t, 16384, 15360, 0.9375\n",
      "38, layer2.0.conv1, layer2.0.conv1,\t\t\t, 32768, 32768, 1.0\n",
      "41, layer2.0.conv2, layer2.0.conv2,\t\t\t, 147456, 100980, 0.684814453125\n",
      "43, layer2.0.conv3, layer2.0.conv3,\t\t\t, 65536, 52224, 0.796875\n",
      "47, layer2.0.downsample.0, layer2.0.downsample.0,\t\t\t, 131072, 131072, 1.0\n",
      "50, layer2.1.conv1, layer2.1.conv1,\t\t\t, 65536, 65536, 1.0\n",
      "53, layer2.1.conv2, layer2.1.conv2,\t\t\t, 147456, 12078, 0.0819091796875\n",
      "55, layer2.1.conv3, layer2.1.conv3,\t\t\t, 65536, 31232, 0.4765625\n",
      "59, layer2.2.conv1, layer2.2.conv1,\t\t\t, 65536, 65536, 1.0\n",
      "62, layer2.2.conv2, layer2.2.conv2,\t\t\t, 147456, 56304, 0.3818359375\n",
      "64, layer2.2.conv3, layer2.2.conv3,\t\t\t, 65536, 47104, 0.71875\n",
      "68, layer2.3.conv1, layer2.3.conv1,\t\t\t, 65536, 65536, 1.0\n",
      "71, layer2.3.conv2, layer2.3.conv2,\t\t\t, 147456, 56700, 0.384521484375\n",
      "73, layer2.3.conv3, layer2.3.conv3,\t\t\t, 65536, 53760, 0.8203125\n",
      "76, reduce, reduce,\t\t\t, 131072, 131072, 1.0\n",
      "123, final_layer, final_layer,\t\t\t, 4352, 4352, 1.0\n",
      "1570240 1038846 0.6615842164255146\n"
     ]
    }
   ],
   "source": [
    "total_orig_conv_wts = 0\n",
    "total_pruned_conv_wts = 0\n",
    "orig_modules = list(model.named_modules())\n",
    "pruned_modules = list(pruned_model.named_modules())\n",
    "layer_wts_before_prune1 = {}\n",
    "layer_wts_after_prune1 = {}\n",
    "for layer_id in range(len(orig_modules)):\n",
    "    m0 = orig_modules[layer_id]\n",
    "    m1 = pruned_modules[layer_id]\n",
    "    # print(layer_id, m0[0], m1[0])\n",
    "    if isinstance(m0[1], nn.Conv2d):\n",
    "\n",
    "        orig_conv_wts = m0[1].weight.data.numel()\n",
    "        total_orig_conv_wts += orig_conv_wts\n",
    "        pruned_conv_wts = m1[1].weight.data.numel()\n",
    "        total_pruned_conv_wts += pruned_conv_wts\n",
    "        print(f\"{layer_id}, {m0[0]}, {m1[0]},\\t\\t\\t, {orig_conv_wts}, {pruned_conv_wts}, {pruned_conv_wts/orig_conv_wts}\")\n",
    "        layer_wts_before_prune1[layer_id] = orig_conv_wts\n",
    "        layer_wts_after_prune1[layer_id] = pruned_conv_wts\n",
    "        \n",
    "        \n",
    "    layer_id += 1\n",
    "print(total_orig_conv_wts ,total_pruned_conv_wts, total_pruned_conv_wts/total_orig_conv_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7f7e2ad6-65cb-4bd3-857c-543b6117220e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 26 artists>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(figsize=(15,10))\n",
    "plt.bar( range(0,len(layer_wts_before_prune1)*5,5), layer_wts_before_prune1.values())\n",
    "plt.bar( range(1,len(layer_wts_after_prune1)*5+1,5), layer_wts_after_prune1.values())\n",
    "plt.bar( range(2,len(layer_wts_after_prune2)*5+2,5), layer_wts_after_prune2.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e9e657f3-ffa9-4e50-a1f9-27224d5c05be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125, 124)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(orig_modules), len(pruned_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63ba5c0-b174-401e-b5df-5b3275805b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "236b3a8c-6787-40cb-ac11-891802282671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==> Add Sine PositionEmbedding~\n",
      "=> init final conv weights from normal distribution\n",
      "=> init .weight as normal(0, 0.001)\n",
      "=> init .bias as 0\n",
      "=> loading pretrained model models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: pos_embedding is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: conv1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: bn1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: bn1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: bn1.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: bn1.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: bn1.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.0.conv1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.0.bn1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.0.bn1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.0.bn1.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.0.bn1.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.0.bn1.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.0.select.indexes is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.0.conv2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.0.bn2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.0.bn2.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.0.bn2.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.0.bn2.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.0.bn2.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.0.conv3.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.0.bn3.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.0.bn3.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.0.bn3.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.0.bn3.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.0.bn3.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.0.downsample.0.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.0.downsample.1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.0.downsample.1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.0.downsample.1.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.0.downsample.1.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.0.downsample.1.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.1.conv1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.1.bn1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.1.bn1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.1.bn1.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.1.bn1.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.1.bn1.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.1.select.indexes is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.1.conv2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.1.bn2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.1.bn2.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.1.bn2.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.1.bn2.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.1.bn2.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.1.conv3.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.1.bn3.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.1.bn3.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.1.bn3.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.1.bn3.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.1.bn3.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.2.conv1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.2.bn1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.2.bn1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.2.bn1.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.2.bn1.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.2.bn1.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.2.select.indexes is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.2.conv2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.2.bn2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.2.bn2.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.2.bn2.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.2.bn2.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.2.bn2.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.2.conv3.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.2.bn3.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.2.bn3.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.2.bn3.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.2.bn3.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer1.2.bn3.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.0.conv1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.0.bn1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.0.bn1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.0.bn1.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.0.bn1.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.0.bn1.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.0.select.indexes is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.0.conv2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.0.bn2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.0.bn2.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.0.bn2.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.0.bn2.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.0.bn2.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.0.conv3.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.0.bn3.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.0.bn3.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.0.bn3.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.0.bn3.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.0.bn3.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.0.downsample.0.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.0.downsample.1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.0.downsample.1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.0.downsample.1.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.0.downsample.1.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.0.downsample.1.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.1.conv1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.1.bn1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.1.bn1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.1.bn1.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.1.bn1.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.1.bn1.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.1.select.indexes is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.1.conv2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.1.bn2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.1.bn2.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.1.bn2.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.1.bn2.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.1.bn2.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.1.conv3.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.1.bn3.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.1.bn3.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.1.bn3.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.1.bn3.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.1.bn3.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.2.conv1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.2.bn1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.2.bn1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.2.bn1.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.2.bn1.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.2.bn1.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.2.select.indexes is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.2.conv2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.2.bn2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.2.bn2.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.2.bn2.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.2.bn2.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.2.bn2.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.2.conv3.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.2.bn3.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.2.bn3.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.2.bn3.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.2.bn3.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.2.bn3.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.3.conv1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.3.bn1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.3.bn1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.3.bn1.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.3.bn1.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.3.bn1.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.3.select.indexes is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.3.conv2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.3.bn2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.3.bn2.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.3.bn2.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.3.bn2.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.3.bn2.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.3.conv3.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.3.bn3.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.3.bn3.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.3.bn3.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.3.bn3.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: layer2.3.bn3.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: reduce.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.0.self_attn.in_proj_weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.0.self_attn.in_proj_bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.0.self_attn.out_proj.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.0.self_attn.out_proj.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.0.linear1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.0.linear1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.0.linear2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.0.linear2.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.0.norm1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.0.norm1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.0.norm2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.0.norm2.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.1.self_attn.in_proj_weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.1.self_attn.in_proj_bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.1.self_attn.out_proj.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.1.self_attn.out_proj.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.1.linear1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.1.linear1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.1.linear2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.1.linear2.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.1.norm1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.1.norm1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.1.norm2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.1.norm2.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.2.self_attn.in_proj_weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.2.self_attn.in_proj_bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.2.self_attn.out_proj.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.2.self_attn.out_proj.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.2.linear1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.2.linear1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.2.linear2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.2.linear2.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.2.norm1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.2.norm1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.2.norm2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.2.norm2.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.3.self_attn.in_proj_weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.3.self_attn.in_proj_bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.3.self_attn.out_proj.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.3.self_attn.out_proj.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.3.linear1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.3.linear1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.3.linear2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.3.linear2.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.3.norm1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.3.norm1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.3.norm2.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: global_encoder.layers.3.norm2.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: deconv_layers.0.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: deconv_layers.1.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: deconv_layers.1.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: deconv_layers.1.running_mean is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: deconv_layers.1.running_var is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: deconv_layers.1.num_batches_tracked is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: final_layer.weight is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n",
      ":: final_layer.bias is loaded from models/pytorch/transpose_r_modelfile_finetune_iter2.pth\n"
     ]
    }
   ],
   "source": [
    "pruned_model2 = eval('models.'+cfg.MODEL.NAME+'.get_pose_net')(\n",
    "    cfg, is_train=False, nw_cfg = nw_cfg_2\n",
    ")\n",
    "pruned_model2.init_weights(os.path.join(models_dir, model_finetune2_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b276ca76-7d0b-45c3-8b76-122478a0ac3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, conv1, conv1,\t\t\t, 9408, 9408, 1.0\n",
      "7, layer1.0.conv1, layer1.0.conv1,\t\t\t, 4096, 4096, 1.0\n",
      "10, layer1.0.conv2, layer1.0.conv2,\t\t\t, 36864, 4590, 0.12451171875\n",
      "12, layer1.0.conv3, layer1.0.conv3,\t\t\t, 16384, 4352, 0.265625\n",
      "16, layer1.0.downsample.0, layer1.0.downsample.0,\t\t\t, 16384, 16384, 1.0\n",
      "19, layer1.1.conv1, layer1.1.conv1,\t\t\t, 16384, 16384, 1.0\n",
      "22, layer1.1.conv2, layer1.1.conv2,\t\t\t, 36864, 6048, 0.1640625\n",
      "24, layer1.1.conv3, layer1.1.conv3,\t\t\t, 16384, 7168, 0.4375\n",
      "28, layer1.2.conv1, layer1.2.conv1,\t\t\t, 16384, 16384, 1.0\n",
      "31, layer1.2.conv2, layer1.2.conv2,\t\t\t, 36864, 17955, 0.487060546875\n",
      "33, layer1.2.conv3, layer1.2.conv3,\t\t\t, 16384, 14592, 0.890625\n",
      "38, layer2.0.conv1, layer2.0.conv1,\t\t\t, 32768, 32768, 1.0\n",
      "41, layer2.0.conv2, layer2.0.conv2,\t\t\t, 147456, 76626, 0.5196533203125\n",
      "43, layer2.0.conv3, layer2.0.conv3,\t\t\t, 65536, 44032, 0.671875\n",
      "47, layer2.0.downsample.0, layer2.0.downsample.0,\t\t\t, 131072, 131072, 1.0\n",
      "50, layer2.1.conv1, layer2.1.conv1,\t\t\t, 65536, 65536, 1.0\n",
      "53, layer2.1.conv2, layer2.1.conv2,\t\t\t, 147456, 5796, 0.039306640625\n",
      "55, layer2.1.conv3, layer2.1.conv3,\t\t\t, 65536, 23552, 0.359375\n",
      "59, layer2.2.conv1, layer2.2.conv1,\t\t\t, 65536, 65536, 1.0\n",
      "62, layer2.2.conv2, layer2.2.conv2,\t\t\t, 147456, 38610, 0.2618408203125\n",
      "64, layer2.2.conv3, layer2.2.conv3,\t\t\t, 65536, 39936, 0.609375\n",
      "68, layer2.3.conv1, layer2.3.conv1,\t\t\t, 65536, 65536, 1.0\n",
      "71, layer2.3.conv2, layer2.3.conv2,\t\t\t, 147456, 42930, 0.2911376953125\n",
      "73, layer2.3.conv3, layer2.3.conv3,\t\t\t, 65536, 46080, 0.703125\n",
      "76, reduce, reduce,\t\t\t, 131072, 131072, 1.0\n",
      "123, final_layer, final_layer,\t\t\t, 4352, 4352, 1.0\n",
      "1570240 930795 0.5927724424291828\n"
     ]
    }
   ],
   "source": [
    "total_orig_conv_wts = 0\n",
    "total_pruned_conv_wts = 0\n",
    "orig_modules = list(model.named_modules())\n",
    "pruned_modules = list(pruned_model2.named_modules())\n",
    "\n",
    "layer_wts_after_prune2 = {}\n",
    "for layer_id in range(len(orig_modules)):\n",
    "    m0 = orig_modules[layer_id]\n",
    "    m1 = pruned_modules[layer_id]\n",
    "    # print(layer_id, m0[0], m1[0])\n",
    "    if isinstance(m0[1], nn.Conv2d):\n",
    "\n",
    "        orig_conv_wts = m0[1].weight.data.numel()\n",
    "        total_orig_conv_wts += orig_conv_wts\n",
    "        pruned_conv_wts = m1[1].weight.data.numel()\n",
    "        total_pruned_conv_wts += pruned_conv_wts\n",
    "        print(f\"{layer_id}, {m0[0]}, {m1[0]},\\t\\t\\t, {orig_conv_wts}, {pruned_conv_wts}, {pruned_conv_wts/orig_conv_wts}\")\n",
    "        # layer_wts_before_prune1[layer_id] = orig_conv_wts\n",
    "        layer_wts_after_prune2[layer_id] = pruned_conv_wts\n",
    "        \n",
    "        \n",
    "    layer_id += 1\n",
    "print(total_orig_conv_wts ,total_pruned_conv_wts, total_pruned_conv_wts/total_orig_conv_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18641f4-afe5-43a0-b362-e2afb3e19f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bbb4ca-c47a-4e47-99c8-f7e399e62b38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80595f7-d517-408a-a54c-88c81d5a77ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
